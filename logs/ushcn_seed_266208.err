/home/gujuluwa/TPatchGNN/t-PatchGNN/tPatchGNN/run_models.py
2025-10-14 21:51:12
tPatchGNN/run_models.py --dataset ushcn --state def --history 24 --patience 10 --batch_size 192 --lr 1e-3 --patch_size 2 --stride 2 --nhead 1 --tf_layer 1 --nlayer 1 --te_dim 10 --node_dim 10 --hid_dim 32 --outlayer Linear --seed 1 --gpu 0
Namespace(state='def', n=100000000, hop=1, nhead=1, tf_layer=1, nlayer=1, epoch=1000, patience=10, history=24, patch_size=2.0, stride=2.0, logmode='a', lr=0.001, w_decay=0.0, batch_size=192, save='experiments/', load=None, seed=1, dataset='ushcn', quantization=0.0, model='tPatchGNN', outlayer='Linear', hid_dim=32, te_dim=10, node_dim=10, gpu='0', npatch=12, device=device(type='cuda'), PID=2756328, n_months=48, pred_window=1, ndim=5)
- Epoch 000, ExpID 35642
Train - Loss (one batch): 0.24241
Val - Loss, MSE, RMSE, MAE, MAPE: 0.83442, 0.83442, 0.91347, 0.35880, -66.17%
Test - Best epoch, Loss, MSE, RMSE, MAE, MAPE: 0, 0.52511, 0.52511, 0.72464, 0.33095, -64.24%
Time spent: 11.45s
- Epoch 001, ExpID 35642
Train - Loss (one batch): 0.60711
Val - Loss, MSE, RMSE, MAE, MAPE: 0.80927, 0.80927, 0.89959, 0.33966, -52.36%
Test - Best epoch, Loss, MSE, RMSE, MAE, MAPE: 1, 0.51858, 0.51858, 0.72012, 0.31216, -49.91%
Time spent: 10.24s
- Epoch 002, ExpID 35642
Train - Loss (one batch): 1.28929
Val - Loss, MSE, RMSE, MAE, MAPE: 0.77455, 0.77455, 0.88009, 0.33979, -57.78%
Test - Best epoch, Loss, MSE, RMSE, MAE, MAPE: 2, 0.51167, 0.51167, 0.71531, 0.31274, -55.27%
Time spent: 10.25s
- Epoch 003, ExpID 35642
Train - Loss (one batch): 0.27591
Val - Loss, MSE, RMSE, MAE, MAPE: 0.80805, 0.80805, 0.89891, 0.35498, -47.79%
Test - Best epoch, Loss, MSE, RMSE, MAE, MAPE: 2, 0.51167, 0.51167, 0.71531, 0.31274, -55.27%
Time spent: 8.52s
- Epoch 004, ExpID 35642
Train - Loss (one batch): 0.44646
Val - Loss, MSE, RMSE, MAE, MAPE: 0.76611, 0.76611, 0.87527, 0.33869, -57.26%
Test - Best epoch, Loss, MSE, RMSE, MAE, MAPE: 4, 0.50471, 0.50471, 0.71043, 0.31348, -55.88%
Time spent: 10.30s
- Epoch 005, ExpID 35642
Train - Loss (one batch): 0.48787
Val - Loss, MSE, RMSE, MAE, MAPE: 0.74167, 0.74167, 0.86120, 0.33447, -52.62%
Test - Best epoch, Loss, MSE, RMSE, MAE, MAPE: 5, 0.50340, 0.50340, 0.70951, 0.30938, -51.33%
Time spent: 10.29s
- Epoch 006, ExpID 35642
Train - Loss (one batch): 0.45079
Val - Loss, MSE, RMSE, MAE, MAPE: 0.72106, 0.72106, 0.84915, 0.37014, -79.94%
Test - Best epoch, Loss, MSE, RMSE, MAE, MAPE: 6, 0.50605, 0.50605, 0.71137, 0.34627, -77.67%
Time spent: 10.26s
- Epoch 007, ExpID 35642
Train - Loss (one batch): 0.81847
Val - Loss, MSE, RMSE, MAE, MAPE: 0.71644, 0.71644, 0.84643, 0.35071, -68.25%
Test - Best epoch, Loss, MSE, RMSE, MAE, MAPE: 7, 0.50045, 0.50045, 0.70743, 0.32691, -65.77%
Time spent: 10.23s
- Epoch 008, ExpID 35642
Train - Loss (one batch): 0.35572
Val - Loss, MSE, RMSE, MAE, MAPE: 0.70908, 0.70908, 0.84207, 0.33919, -59.53%
Test - Best epoch, Loss, MSE, RMSE, MAE, MAPE: 8, 0.50187, 0.50187, 0.70843, 0.31483, -57.26%
Time spent: 10.30s
- Epoch 009, ExpID 35642
Train - Loss (one batch): 0.29799
Val - Loss, MSE, RMSE, MAE, MAPE: 0.69223, 0.69223, 0.83200, 0.32794, -53.34%
Test - Best epoch, Loss, MSE, RMSE, MAE, MAPE: 9, 0.49867, 0.49867, 0.70616, 0.30510, -50.73%
Time spent: 10.30s
- Epoch 010, ExpID 35642
Train - Loss (one batch): 0.27309
Val - Loss, MSE, RMSE, MAE, MAPE: 0.72031, 0.72031, 0.84871, 0.36095, -74.72%
Test - Best epoch, Loss, MSE, RMSE, MAE, MAPE: 9, 0.49867, 0.49867, 0.70616, 0.30510, -50.73%
Time spent: 8.51s
- Epoch 011, ExpID 35642
Train - Loss (one batch): 0.87907
Val - Loss, MSE, RMSE, MAE, MAPE: 0.70301, 0.70301, 0.83846, 0.35048, -71.41%
Test - Best epoch, Loss, MSE, RMSE, MAE, MAPE: 9, 0.49867, 0.49867, 0.70616, 0.30510, -50.73%
Time spent: 8.50s
- Epoch 012, ExpID 35642
Train - Loss (one batch): 0.37661
Val - Loss, MSE, RMSE, MAE, MAPE: 0.69524, 0.69524, 0.83381, 0.32742, -53.31%
Test - Best epoch, Loss, MSE, RMSE, MAE, MAPE: 9, 0.49867, 0.49867, 0.70616, 0.30510, -50.73%
Time spent: 8.51s
- Epoch 013, ExpID 35642
Train - Loss (one batch): 0.48991
Val - Loss, MSE, RMSE, MAE, MAPE: 0.69104, 0.69104, 0.83129, 0.33889, -64.53%
Test - Best epoch, Loss, MSE, RMSE, MAE, MAPE: 13, 0.49760, 0.49760, 0.70541, 0.31562, -60.60%
Time spent: 10.23s
- Epoch 014, ExpID 35642
Train - Loss (one batch): 0.57454
Val - Loss, MSE, RMSE, MAE, MAPE: 0.69207, 0.69207, 0.83191, 0.33679, -62.62%
Test - Best epoch, Loss, MSE, RMSE, MAE, MAPE: 13, 0.49760, 0.49760, 0.70541, 0.31562, -60.60%
Time spent: 8.56s
- Epoch 015, ExpID 35642
Train - Loss (one batch): 0.27205
Val - Loss, MSE, RMSE, MAE, MAPE: 0.70938, 0.70938, 0.84225, 0.33220, -53.38%
Test - Best epoch, Loss, MSE, RMSE, MAE, MAPE: 13, 0.49760, 0.49760, 0.70541, 0.31562, -60.60%
Time spent: 8.49s
- Epoch 016, ExpID 35642
Train - Loss (one batch): 0.42221
Val - Loss, MSE, RMSE, MAE, MAPE: 0.69731, 0.69731, 0.83505, 0.31707, -49.62%
Test - Best epoch, Loss, MSE, RMSE, MAE, MAPE: 13, 0.49760, 0.49760, 0.70541, 0.31562, -60.60%
Time spent: 8.52s
- Epoch 017, ExpID 35642
Train - Loss (one batch): 0.42343
Val - Loss, MSE, RMSE, MAE, MAPE: 0.69403, 0.69403, 0.83308, 0.33555, -63.39%
Test - Best epoch, Loss, MSE, RMSE, MAE, MAPE: 13, 0.49760, 0.49760, 0.70541, 0.31562, -60.60%
Time spent: 8.50s
- Epoch 018, ExpID 35642
Train - Loss (one batch): 0.54062
Val - Loss, MSE, RMSE, MAE, MAPE: 0.69157, 0.69157, 0.83161, 0.31684, -48.29%
Test - Best epoch, Loss, MSE, RMSE, MAE, MAPE: 13, 0.49760, 0.49760, 0.70541, 0.31562, -60.60%
Time spent: 8.54s
- Epoch 019, ExpID 35642
Train - Loss (one batch): 0.25533
Val - Loss, MSE, RMSE, MAE, MAPE: 0.71340, 0.71340, 0.84463, 0.34071, -58.77%
Test - Best epoch, Loss, MSE, RMSE, MAE, MAPE: 13, 0.49760, 0.49760, 0.70541, 0.31562, -60.60%
Time spent: 8.57s
- Epoch 020, ExpID 35642
Train - Loss (one batch): 0.40183
Val - Loss, MSE, RMSE, MAE, MAPE: 0.68626, 0.68626, 0.82841, 0.33008, -59.14%
Test - Best epoch, Loss, MSE, RMSE, MAE, MAPE: 20, 0.49512, 0.49512, 0.70365, 0.30654, -55.16%
Time spent: 10.24s
- Epoch 021, ExpID 35642
Train - Loss (one batch): 0.42127
Val - Loss, MSE, RMSE, MAE, MAPE: 0.69628, 0.69628, 0.83443, 0.33090, -60.35%
Test - Best epoch, Loss, MSE, RMSE, MAE, MAPE: 20, 0.49512, 0.49512, 0.70365, 0.30654, -55.16%
Time spent: 8.51s
- Epoch 022, ExpID 35642
Train - Loss (one batch): 0.27291
Val - Loss, MSE, RMSE, MAE, MAPE: 0.69691, 0.69691, 0.83481, 0.32374, -57.66%
Test - Best epoch, Loss, MSE, RMSE, MAE, MAPE: 20, 0.49512, 0.49512, 0.70365, 0.30654, -55.16%
Time spent: 8.50s
- Epoch 023, ExpID 35642
Train - Loss (one batch): 0.50182
Val - Loss, MSE, RMSE, MAE, MAPE: 0.69536, 0.69536, 0.83388, 0.32979, -58.47%
Test - Best epoch, Loss, MSE, RMSE, MAE, MAPE: 20, 0.49512, 0.49512, 0.70365, 0.30654, -55.16%
Time spent: 8.49s
- Epoch 024, ExpID 35642
Train - Loss (one batch): 0.42833
Val - Loss, MSE, RMSE, MAE, MAPE: 0.69560, 0.69560, 0.83403, 0.33629, -62.20%
Test - Best epoch, Loss, MSE, RMSE, MAE, MAPE: 20, 0.49512, 0.49512, 0.70365, 0.30654, -55.16%
Time spent: 8.53s
- Epoch 025, ExpID 35642
Train - Loss (one batch): 0.85158
Val - Loss, MSE, RMSE, MAE, MAPE: 0.67809, 0.67809, 0.82346, 0.32680, -55.15%
Test - Best epoch, Loss, MSE, RMSE, MAE, MAPE: 25, 0.49435, 0.49435, 0.70310, 0.30399, -52.60%
Time spent: 10.25s
- Epoch 026, ExpID 35642
Train - Loss (one batch): 0.41660
Val - Loss, MSE, RMSE, MAE, MAPE: 0.68412, 0.68412, 0.82712, 0.32894, -55.96%
Test - Best epoch, Loss, MSE, RMSE, MAE, MAPE: 25, 0.49435, 0.49435, 0.70310, 0.30399, -52.60%
Time spent: 8.52s
- Epoch 027, ExpID 35642
Train - Loss (one batch): 0.73924
Val - Loss, MSE, RMSE, MAE, MAPE: 0.67232, 0.67232, 0.81995, 0.32644, -58.85%
Test - Best epoch, Loss, MSE, RMSE, MAE, MAPE: 27, 0.49156, 0.49156, 0.70111, 0.30340, -54.63%
Time spent: 10.26s
- Epoch 028, ExpID 35642
Train - Loss (one batch): 0.42966
Val - Loss, MSE, RMSE, MAE, MAPE: 0.69560, 0.69560, 0.83403, 0.36402, -75.56%
Test - Best epoch, Loss, MSE, RMSE, MAE, MAPE: 27, 0.49156, 0.49156, 0.70111, 0.30340, -54.63%
Time spent: 8.51s
- Epoch 029, ExpID 35642
Train - Loss (one batch): 0.26001
Val - Loss, MSE, RMSE, MAE, MAPE: 0.67343, 0.67343, 0.82063, 0.33152, -60.12%
Test - Best epoch, Loss, MSE, RMSE, MAE, MAPE: 27, 0.49156, 0.49156, 0.70111, 0.30340, -54.63%
Time spent: 8.58s
- Epoch 030, ExpID 35642
Train - Loss (one batch): 0.51777
Val - Loss, MSE, RMSE, MAE, MAPE: 0.69779, 0.69779, 0.83534, 0.32171, -53.93%
Test - Best epoch, Loss, MSE, RMSE, MAE, MAPE: 27, 0.49156, 0.49156, 0.70111, 0.30340, -54.63%
Time spent: 8.51s
- Epoch 031, ExpID 35642
Train - Loss (one batch): 0.30674
Val - Loss, MSE, RMSE, MAE, MAPE: 0.68175, 0.68175, 0.82568, 0.33216, -63.39%
Test - Best epoch, Loss, MSE, RMSE, MAE, MAPE: 27, 0.49156, 0.49156, 0.70111, 0.30340, -54.63%
Time spent: 8.49s
- Epoch 032, ExpID 35642
Train - Loss (one batch): 0.40497
Val - Loss, MSE, RMSE, MAE, MAPE: 0.66251, 0.66251, 0.81395, 0.30866, -47.42%
Test - Best epoch, Loss, MSE, RMSE, MAE, MAPE: 32, 0.49677, 0.49677, 0.70482, 0.28508, -44.14%
Time spent: 10.25s
- Epoch 033, ExpID 35642
Train - Loss (one batch): 0.32878
Val - Loss, MSE, RMSE, MAE, MAPE: 0.69339, 0.69339, 0.83270, 0.32495, -56.78%
Test - Best epoch, Loss, MSE, RMSE, MAE, MAPE: 32, 0.49677, 0.49677, 0.70482, 0.28508, -44.14%
Time spent: 8.50s
- Epoch 034, ExpID 35642
Train - Loss (one batch): 0.15274
Val - Loss, MSE, RMSE, MAE, MAPE: 0.67260, 0.67260, 0.82012, 0.31332, -48.09%
Test - Best epoch, Loss, MSE, RMSE, MAE, MAPE: 32, 0.49677, 0.49677, 0.70482, 0.28508, -44.14%
Time spent: 8.54s
- Epoch 035, ExpID 35642
Train - Loss (one batch): 0.45815
Val - Loss, MSE, RMSE, MAE, MAPE: 0.66967, 0.66967, 0.81833, 0.31434, -53.09%
Test - Best epoch, Loss, MSE, RMSE, MAE, MAPE: 32, 0.49677, 0.49677, 0.70482, 0.28508, -44.14%
Time spent: 8.59s
- Epoch 036, ExpID 35642
Train - Loss (one batch): 0.24704
Val - Loss, MSE, RMSE, MAE, MAPE: 0.66862, 0.66862, 0.81769, 0.32409, -56.57%
Test - Best epoch, Loss, MSE, RMSE, MAE, MAPE: 32, 0.49677, 0.49677, 0.70482, 0.28508, -44.14%
Time spent: 8.53s
- Epoch 037, ExpID 35642
Train - Loss (one batch): 0.43063
Val - Loss, MSE, RMSE, MAE, MAPE: 0.66974, 0.66974, 0.81838, 0.32617, -56.71%
Test - Best epoch, Loss, MSE, RMSE, MAE, MAPE: 32, 0.49677, 0.49677, 0.70482, 0.28508, -44.14%
Time spent: 8.50s
- Epoch 038, ExpID 35642
Train - Loss (one batch): 0.36377
Val - Loss, MSE, RMSE, MAE, MAPE: 0.67487, 0.67487, 0.82151, 0.34719, -73.62%
Test - Best epoch, Loss, MSE, RMSE, MAE, MAPE: 32, 0.49677, 0.49677, 0.70482, 0.28508, -44.14%
Time spent: 8.49s
- Epoch 039, ExpID 35642
Train - Loss (one batch): 0.23816
Val - Loss, MSE, RMSE, MAE, MAPE: 0.69855, 0.69855, 0.83579, 0.32765, -58.90%
Test - Best epoch, Loss, MSE, RMSE, MAE, MAPE: 32, 0.49677, 0.49677, 0.70482, 0.28508, -44.14%
Time spent: 8.58s
- Epoch 040, ExpID 35642
Train - Loss (one batch): 0.32311
Val - Loss, MSE, RMSE, MAE, MAPE: 0.69726, 0.69726, 0.83502, 0.32704, -58.70%
Test - Best epoch, Loss, MSE, RMSE, MAE, MAPE: 32, 0.49677, 0.49677, 0.70482, 0.28508, -44.14%
Time spent: 8.60s
- Epoch 041, ExpID 35642
Train - Loss (one batch): 0.33776
Val - Loss, MSE, RMSE, MAE, MAPE: 0.68811, 0.68811, 0.82953, 0.32394, -56.81%
Test - Best epoch, Loss, MSE, RMSE, MAE, MAPE: 32, 0.49677, 0.49677, 0.70482, 0.28508, -44.14%
Time spent: 8.55s
- Epoch 042, ExpID 35642
Train - Loss (one batch): 0.76987
Val - Loss, MSE, RMSE, MAE, MAPE: 0.67873, 0.67873, 0.82385, 0.36032, -81.21%
Test - Best epoch, Loss, MSE, RMSE, MAE, MAPE: 32, 0.49677, 0.49677, 0.70482, 0.28508, -44.14%
Time spent: 8.50s
/home/gujuluwa/TPatchGNN/t-PatchGNN/tPatchGNN/run_models.py
2025-10-14 21:57:54
tPatchGNN/run_models.py --dataset ushcn --state def --history 24 --patience 10 --batch_size 192 --lr 1e-3 --patch_size 2 --stride 2 --nhead 1 --tf_layer 1 --nlayer 1 --te_dim 10 --node_dim 10 --hid_dim 32 --outlayer Linear --seed 2 --gpu 0
Namespace(state='def', n=100000000, hop=1, nhead=1, tf_layer=1, nlayer=1, epoch=1000, patience=10, history=24, patch_size=2.0, stride=2.0, logmode='a', lr=0.001, w_decay=0.0, batch_size=192, save='experiments/', load=None, seed=2, dataset='ushcn', quantization=0.0, model='tPatchGNN', outlayer='Linear', hid_dim=32, te_dim=10, node_dim=10, gpu='0', npatch=12, device=device(type='cuda'), PID=2756889, n_months=48, pred_window=1, ndim=5)
- Epoch 000, ExpID 77597
Train - Loss (one batch): 0.79333
Val - Loss, MSE, RMSE, MAE, MAPE: 0.84012, 0.84012, 0.91658, 0.35125, -56.84%
Test - Best epoch, Loss, MSE, RMSE, MAE, MAPE: 0, 0.53157, 0.53157, 0.72909, 0.32583, -56.14%
Time spent: 11.09s
- Epoch 001, ExpID 77597
Train - Loss (one batch): 0.50627
Val - Loss, MSE, RMSE, MAE, MAPE: 0.78259, 0.78259, 0.88464, 0.35588, -62.58%
Test - Best epoch, Loss, MSE, RMSE, MAE, MAPE: 1, 0.51729, 0.51729, 0.71923, 0.33165, -61.06%
Time spent: 10.40s
- Epoch 002, ExpID 77597
Train - Loss (one batch): 0.28741
Val - Loss, MSE, RMSE, MAE, MAPE: 0.77355, 0.77355, 0.87952, 0.34402, -57.10%
Test - Best epoch, Loss, MSE, RMSE, MAE, MAPE: 2, 0.51077, 0.51077, 0.71469, 0.31983, -56.03%
Time spent: 10.37s
- Epoch 003, ExpID 77597
Train - Loss (one batch): 0.72957
Val - Loss, MSE, RMSE, MAE, MAPE: 0.75098, 0.75098, 0.86659, 0.35136, -61.76%
Test - Best epoch, Loss, MSE, RMSE, MAE, MAPE: 3, 0.51482, 0.51482, 0.71751, 0.32735, -59.74%
Time spent: 10.36s
- Epoch 004, ExpID 77597
Train - Loss (one batch): 0.37443
Val - Loss, MSE, RMSE, MAE, MAPE: 0.71564, 0.71564, 0.84595, 0.33730, -56.14%
Test - Best epoch, Loss, MSE, RMSE, MAE, MAPE: 4, 0.50641, 0.50641, 0.71162, 0.31508, -55.42%
Time spent: 10.39s
- Epoch 005, ExpID 77597
Train - Loss (one batch): 0.60290
Val - Loss, MSE, RMSE, MAE, MAPE: 0.72231, 0.72231, 0.84989, 0.34996, -64.02%
Test - Best epoch, Loss, MSE, RMSE, MAE, MAPE: 4, 0.50641, 0.50641, 0.71162, 0.31508, -55.42%
Time spent: 8.64s
- Epoch 006, ExpID 77597
Train - Loss (one batch): 0.54275
Val - Loss, MSE, RMSE, MAE, MAPE: 0.73815, 0.73815, 0.85916, 0.36997, -80.74%
Test - Best epoch, Loss, MSE, RMSE, MAE, MAPE: 4, 0.50641, 0.50641, 0.71162, 0.31508, -55.42%
Time spent: 8.57s
- Epoch 007, ExpID 77597
Train - Loss (one batch): 0.58659
Val - Loss, MSE, RMSE, MAE, MAPE: 0.71641, 0.71641, 0.84641, 0.33281, -55.96%
Test - Best epoch, Loss, MSE, RMSE, MAE, MAPE: 4, 0.50641, 0.50641, 0.71162, 0.31508, -55.42%
Time spent: 8.58s
- Epoch 008, ExpID 77597
Train - Loss (one batch): 0.34803
Val - Loss, MSE, RMSE, MAE, MAPE: 0.70709, 0.70709, 0.84089, 0.33576, -55.70%
Test - Best epoch, Loss, MSE, RMSE, MAE, MAPE: 8, 0.50738, 0.50738, 0.71230, 0.31416, -54.73%
Time spent: 10.31s
- Epoch 009, ExpID 77597
Train - Loss (one batch): 0.36182
Val - Loss, MSE, RMSE, MAE, MAPE: 0.70400, 0.70400, 0.83905, 0.32592, -49.35%
Test - Best epoch, Loss, MSE, RMSE, MAE, MAPE: 9, 0.50042, 0.50042, 0.70741, 0.30325, -48.70%
Time spent: 10.38s
- Epoch 010, ExpID 77597
Train - Loss (one batch): 0.38987
Val - Loss, MSE, RMSE, MAE, MAPE: 0.71950, 0.71950, 0.84823, 0.33435, -58.14%
Test - Best epoch, Loss, MSE, RMSE, MAE, MAPE: 9, 0.50042, 0.50042, 0.70741, 0.30325, -48.70%
Time spent: 8.64s
- Epoch 011, ExpID 77597
Train - Loss (one batch): 0.39484
Val - Loss, MSE, RMSE, MAE, MAPE: 0.69619, 0.69619, 0.83438, 0.33879, -62.22%
Test - Best epoch, Loss, MSE, RMSE, MAE, MAPE: 11, 0.50226, 0.50226, 0.70870, 0.31758, -60.89%
Time spent: 10.47s
- Epoch 012, ExpID 77597
Train - Loss (one batch): 0.22703
Val - Loss, MSE, RMSE, MAE, MAPE: 0.67341, 0.67341, 0.82062, 0.33040, -57.54%
Test - Best epoch, Loss, MSE, RMSE, MAE, MAPE: 12, 0.50492, 0.50492, 0.71058, 0.30881, -56.36%
Time spent: 10.32s
- Epoch 013, ExpID 77597
Train - Loss (one batch): 0.23334
Val - Loss, MSE, RMSE, MAE, MAPE: 0.69669, 0.69669, 0.83468, 0.32848, -57.32%
Test - Best epoch, Loss, MSE, RMSE, MAE, MAPE: 12, 0.50492, 0.50492, 0.71058, 0.30881, -56.36%
Time spent: 8.61s
- Epoch 014, ExpID 77597
Train - Loss (one batch): 0.47809
Val - Loss, MSE, RMSE, MAE, MAPE: 0.67668, 0.67668, 0.82261, 0.34168, -63.17%
Test - Best epoch, Loss, MSE, RMSE, MAE, MAPE: 12, 0.50492, 0.50492, 0.71058, 0.30881, -56.36%
Time spent: 8.62s
- Epoch 015, ExpID 77597
Train - Loss (one batch): 0.38481
Val - Loss, MSE, RMSE, MAE, MAPE: 0.69813, 0.69813, 0.83554, 0.32764, -52.08%
Test - Best epoch, Loss, MSE, RMSE, MAE, MAPE: 12, 0.50492, 0.50492, 0.71058, 0.30881, -56.36%
Time spent: 8.63s
- Epoch 016, ExpID 77597
Train - Loss (one batch): 0.30796
Val - Loss, MSE, RMSE, MAE, MAPE: 0.68869, 0.68869, 0.82988, 0.32696, -55.81%
Test - Best epoch, Loss, MSE, RMSE, MAE, MAPE: 12, 0.50492, 0.50492, 0.71058, 0.30881, -56.36%
Time spent: 8.66s
- Epoch 017, ExpID 77597
Train - Loss (one batch): 0.67572
Val - Loss, MSE, RMSE, MAE, MAPE: 0.72489, 0.72489, 0.85140, 0.31951, -45.20%
Test - Best epoch, Loss, MSE, RMSE, MAE, MAPE: 12, 0.50492, 0.50492, 0.71058, 0.30881, -56.36%
Time spent: 8.55s
- Epoch 018, ExpID 77597
Train - Loss (one batch): 0.32862
Val - Loss, MSE, RMSE, MAE, MAPE: 0.67799, 0.67799, 0.82340, 0.33191, -57.68%
Test - Best epoch, Loss, MSE, RMSE, MAE, MAPE: 12, 0.50492, 0.50492, 0.71058, 0.30881, -56.36%
Time spent: 8.59s
- Epoch 019, ExpID 77597
Train - Loss (one batch): 0.53339
Val - Loss, MSE, RMSE, MAE, MAPE: 0.67489, 0.67489, 0.82152, 0.32156, -53.58%
Test - Best epoch, Loss, MSE, RMSE, MAE, MAPE: 12, 0.50492, 0.50492, 0.71058, 0.30881, -56.36%
Time spent: 8.57s
- Epoch 020, ExpID 77597
Train - Loss (one batch): 0.35477
Val - Loss, MSE, RMSE, MAE, MAPE: 0.67712, 0.67712, 0.82287, 0.32799, -57.18%
Test - Best epoch, Loss, MSE, RMSE, MAE, MAPE: 12, 0.50492, 0.50492, 0.71058, 0.30881, -56.36%
Time spent: 8.54s
- Epoch 021, ExpID 77597
Train - Loss (one batch): 0.80477
Val - Loss, MSE, RMSE, MAE, MAPE: 0.68138, 0.68138, 0.82545, 0.32716, -56.49%
Test - Best epoch, Loss, MSE, RMSE, MAE, MAPE: 12, 0.50492, 0.50492, 0.71058, 0.30881, -56.36%
Time spent: 8.65s
- Epoch 022, ExpID 77597
Train - Loss (one batch): 0.33613
Val - Loss, MSE, RMSE, MAE, MAPE: 0.69462, 0.69462, 0.83344, 0.32170, -51.24%
Test - Best epoch, Loss, MSE, RMSE, MAE, MAPE: 12, 0.50492, 0.50492, 0.71058, 0.30881, -56.36%
Time spent: 8.60s
/home/gujuluwa/TPatchGNN/t-PatchGNN/tPatchGNN/run_models.py
2025-10-14 22:01:38
tPatchGNN/run_models.py --dataset ushcn --state def --history 24 --patience 10 --batch_size 192 --lr 1e-3 --patch_size 2 --stride 2 --nhead 1 --tf_layer 1 --nlayer 1 --te_dim 10 --node_dim 10 --hid_dim 32 --outlayer Linear --seed 3 --gpu 0
Namespace(state='def', n=100000000, hop=1, nhead=1, tf_layer=1, nlayer=1, epoch=1000, patience=10, history=24, patch_size=2.0, stride=2.0, logmode='a', lr=0.001, w_decay=0.0, batch_size=192, save='experiments/', load=None, seed=3, dataset='ushcn', quantization=0.0, model='tPatchGNN', outlayer='Linear', hid_dim=32, te_dim=10, node_dim=10, gpu='0', npatch=12, device=device(type='cuda'), PID=2757186, n_months=48, pred_window=1, ndim=5)
- Epoch 000, ExpID 70530
Train - Loss (one batch): 0.75472
Val - Loss, MSE, RMSE, MAE, MAPE: 0.84604, 0.84604, 0.91981, 0.35374, -63.61%
Test - Best epoch, Loss, MSE, RMSE, MAE, MAPE: 0, 0.52121, 0.52121, 0.72195, 0.32716, -61.76%
Time spent: 10.99s
- Epoch 001, ExpID 70530
Train - Loss (one batch): 0.51099
Val - Loss, MSE, RMSE, MAE, MAPE: 0.78781, 0.78781, 0.88759, 0.34844, -63.45%
Test - Best epoch, Loss, MSE, RMSE, MAE, MAPE: 1, 0.51722, 0.51722, 0.71918, 0.32260, -60.23%
Time spent: 10.32s
- Epoch 002, ExpID 70530
Train - Loss (one batch): 0.62678
Val - Loss, MSE, RMSE, MAE, MAPE: 0.76800, 0.76800, 0.87636, 0.36748, -74.08%
Test - Best epoch, Loss, MSE, RMSE, MAE, MAPE: 2, 0.52124, 0.52124, 0.72197, 0.34277, -71.05%
Time spent: 10.27s
- Epoch 003, ExpID 70530
Train - Loss (one batch): 0.74472
Val - Loss, MSE, RMSE, MAE, MAPE: 0.74932, 0.74932, 0.86563, 0.35533, -72.82%
Test - Best epoch, Loss, MSE, RMSE, MAE, MAPE: 3, 0.51018, 0.51018, 0.71427, 0.33152, -69.82%
Time spent: 10.20s
- Epoch 004, ExpID 70530
Train - Loss (one batch): 0.25089
Val - Loss, MSE, RMSE, MAE, MAPE: 0.75050, 0.75050, 0.86631, 0.33359, -48.23%
Test - Best epoch, Loss, MSE, RMSE, MAE, MAPE: 3, 0.51018, 0.51018, 0.71427, 0.33152, -69.82%
Time spent: 8.49s
- Epoch 005, ExpID 70530
Train - Loss (one batch): 0.44246
Val - Loss, MSE, RMSE, MAE, MAPE: 0.72928, 0.72928, 0.85398, 0.33465, -56.85%
Test - Best epoch, Loss, MSE, RMSE, MAE, MAPE: 5, 0.50962, 0.50962, 0.71387, 0.31115, -54.45%
Time spent: 10.22s
- Epoch 006, ExpID 70530
Train - Loss (one batch): 0.71258
Val - Loss, MSE, RMSE, MAE, MAPE: 0.76926, 0.76926, 0.87708, 0.35291, -68.10%
Test - Best epoch, Loss, MSE, RMSE, MAE, MAPE: 5, 0.50962, 0.50962, 0.71387, 0.31115, -54.45%
Time spent: 8.51s
- Epoch 007, ExpID 70530
Train - Loss (one batch): 0.42831
Val - Loss, MSE, RMSE, MAE, MAPE: 0.71741, 0.71741, 0.84700, 0.33943, -59.48%
Test - Best epoch, Loss, MSE, RMSE, MAE, MAPE: 7, 0.50655, 0.50655, 0.71173, 0.31557, -57.07%
Time spent: 10.31s
- Epoch 008, ExpID 70530
Train - Loss (one batch): 0.46769
Val - Loss, MSE, RMSE, MAE, MAPE: 0.69853, 0.69853, 0.83578, 0.33012, -54.36%
Test - Best epoch, Loss, MSE, RMSE, MAE, MAPE: 8, 0.50701, 0.50701, 0.71205, 0.30714, -51.95%
Time spent: 10.21s
- Epoch 009, ExpID 70530
Train - Loss (one batch): 0.31964
Val - Loss, MSE, RMSE, MAE, MAPE: 0.74272, 0.74272, 0.86181, 0.32990, -52.73%
Test - Best epoch, Loss, MSE, RMSE, MAE, MAPE: 8, 0.50701, 0.50701, 0.71205, 0.30714, -51.95%
Time spent: 8.51s
- Epoch 010, ExpID 70530
Train - Loss (one batch): 0.51855
Val - Loss, MSE, RMSE, MAE, MAPE: 0.72601, 0.72601, 0.85206, 0.34038, -63.01%
Test - Best epoch, Loss, MSE, RMSE, MAE, MAPE: 8, 0.50701, 0.50701, 0.71205, 0.30714, -51.95%
Time spent: 8.49s
- Epoch 011, ExpID 70530
Train - Loss (one batch): 0.44944
Val - Loss, MSE, RMSE, MAE, MAPE: 0.71797, 0.71797, 0.84733, 0.33990, -61.88%
Test - Best epoch, Loss, MSE, RMSE, MAE, MAPE: 8, 0.50701, 0.50701, 0.71205, 0.30714, -51.95%
Time spent: 8.53s
- Epoch 012, ExpID 70530
Train - Loss (one batch): 0.71327
Val - Loss, MSE, RMSE, MAE, MAPE: 0.73301, 0.73301, 0.85616, 0.34455, -65.16%
Test - Best epoch, Loss, MSE, RMSE, MAE, MAPE: 8, 0.50701, 0.50701, 0.71205, 0.30714, -51.95%
Time spent: 8.52s
- Epoch 013, ExpID 70530
Train - Loss (one batch): 0.39729
Val - Loss, MSE, RMSE, MAE, MAPE: 0.73436, 0.73436, 0.85695, 0.33548, -58.13%
Test - Best epoch, Loss, MSE, RMSE, MAE, MAPE: 8, 0.50701, 0.50701, 0.71205, 0.30714, -51.95%
Time spent: 8.46s
- Epoch 014, ExpID 70530
Train - Loss (one batch): 0.57312
Val - Loss, MSE, RMSE, MAE, MAPE: 0.74098, 0.74098, 0.86080, 0.34847, -66.15%
Test - Best epoch, Loss, MSE, RMSE, MAE, MAPE: 8, 0.50701, 0.50701, 0.71205, 0.30714, -51.95%
Time spent: 8.48s
- Epoch 015, ExpID 70530
Train - Loss (one batch): 0.57143
Val - Loss, MSE, RMSE, MAE, MAPE: 0.71312, 0.71312, 0.84446, 0.35674, -73.32%
Test - Best epoch, Loss, MSE, RMSE, MAE, MAPE: 8, 0.50701, 0.50701, 0.71205, 0.30714, -51.95%
Time spent: 8.49s
- Epoch 016, ExpID 70530
Train - Loss (one batch): 0.59571
Val - Loss, MSE, RMSE, MAE, MAPE: 0.73509, 0.73509, 0.85737, 0.34186, -58.59%
Test - Best epoch, Loss, MSE, RMSE, MAE, MAPE: 8, 0.50701, 0.50701, 0.71205, 0.30714, -51.95%
Time spent: 8.48s
- Epoch 017, ExpID 70530
Train - Loss (one batch): 0.36910
Val - Loss, MSE, RMSE, MAE, MAPE: 0.72489, 0.72489, 0.85141, 0.34639, -67.55%
Test - Best epoch, Loss, MSE, RMSE, MAE, MAPE: 8, 0.50701, 0.50701, 0.71205, 0.30714, -51.95%
Time spent: 8.56s
- Epoch 018, ExpID 70530
Train - Loss (one batch): 0.36900
Val - Loss, MSE, RMSE, MAE, MAPE: 0.73775, 0.73775, 0.85892, 0.32808, -50.80%
Test - Best epoch, Loss, MSE, RMSE, MAE, MAPE: 8, 0.50701, 0.50701, 0.71205, 0.30714, -51.95%
Time spent: 8.55s
/home/gujuluwa/TPatchGNN/t-PatchGNN/tPatchGNN/run_models.py
2025-10-14 22:04:42
tPatchGNN/run_models.py --dataset ushcn --state def --history 24 --patience 10 --batch_size 192 --lr 1e-3 --patch_size 2 --stride 2 --nhead 1 --tf_layer 1 --nlayer 1 --te_dim 10 --node_dim 10 --hid_dim 32 --outlayer Linear --seed 4 --gpu 0
Namespace(state='def', n=100000000, hop=1, nhead=1, tf_layer=1, nlayer=1, epoch=1000, patience=10, history=24, patch_size=2.0, stride=2.0, logmode='a', lr=0.001, w_decay=0.0, batch_size=192, save='experiments/', load=None, seed=4, dataset='ushcn', quantization=0.0, model='tPatchGNN', outlayer='Linear', hid_dim=32, te_dim=10, node_dim=10, gpu='0', npatch=12, device=device(type='cuda'), PID=2757459, n_months=48, pred_window=1, ndim=5)
- Epoch 000, ExpID 34852
Train - Loss (one batch): 0.47300
Val - Loss, MSE, RMSE, MAE, MAPE: 0.80999, 0.80999, 0.90000, 0.35276, -67.89%
Test - Best epoch, Loss, MSE, RMSE, MAE, MAPE: 0, 0.51902, 0.51902, 0.72043, 0.32822, -64.25%
Time spent: 11.11s
- Epoch 001, ExpID 34852
Train - Loss (one batch): 0.38856
Val - Loss, MSE, RMSE, MAE, MAPE: 0.78053, 0.78053, 0.88347, 0.34276, -58.38%
Test - Best epoch, Loss, MSE, RMSE, MAE, MAPE: 1, 0.51468, 0.51468, 0.71741, 0.31953, -56.30%
Time spent: 10.44s
- Epoch 002, ExpID 34852
Train - Loss (one batch): 0.45109
Val - Loss, MSE, RMSE, MAE, MAPE: 0.76404, 0.76404, 0.87409, 0.33361, -49.75%
Test - Best epoch, Loss, MSE, RMSE, MAE, MAPE: 2, 0.50997, 0.50997, 0.71412, 0.30952, -47.37%
Time spent: 10.42s
- Epoch 003, ExpID 34852
Train - Loss (one batch): 0.60549
Val - Loss, MSE, RMSE, MAE, MAPE: 0.75007, 0.75007, 0.86607, 0.33260, -58.34%
Test - Best epoch, Loss, MSE, RMSE, MAE, MAPE: 3, 0.50775, 0.50775, 0.71256, 0.30944, -56.22%
Time spent: 10.38s
- Epoch 004, ExpID 34852
Train - Loss (one batch): 0.49973
Val - Loss, MSE, RMSE, MAE, MAPE: 0.74285, 0.74285, 0.86189, 0.34119, -61.19%
Test - Best epoch, Loss, MSE, RMSE, MAE, MAPE: 4, 0.51002, 0.51002, 0.71415, 0.31919, -59.21%
Time spent: 10.35s
- Epoch 005, ExpID 34852
Train - Loss (one batch): 0.33230
Val - Loss, MSE, RMSE, MAE, MAPE: 0.73081, 0.73081, 0.85488, 0.33571, -58.57%
Test - Best epoch, Loss, MSE, RMSE, MAE, MAPE: 5, 0.51105, 0.51105, 0.71488, 0.31428, -58.07%
Time spent: 10.37s
- Epoch 006, ExpID 34852
Train - Loss (one batch): 0.31663
Val - Loss, MSE, RMSE, MAE, MAPE: 0.73301, 0.73301, 0.85616, 0.33401, -51.82%
Test - Best epoch, Loss, MSE, RMSE, MAE, MAPE: 5, 0.51105, 0.51105, 0.71488, 0.31428, -58.07%
Time spent: 8.58s
- Epoch 007, ExpID 34852
Train - Loss (one batch): 0.29702
Val - Loss, MSE, RMSE, MAE, MAPE: 0.72643, 0.72643, 0.85231, 0.32926, -47.73%
Test - Best epoch, Loss, MSE, RMSE, MAE, MAPE: 7, 0.51171, 0.51171, 0.71534, 0.30784, -47.08%
Time spent: 10.42s
- Epoch 008, ExpID 34852
Train - Loss (one batch): 0.40970
Val - Loss, MSE, RMSE, MAE, MAPE: 0.72101, 0.72101, 0.84912, 0.32635, -53.59%
Test - Best epoch, Loss, MSE, RMSE, MAE, MAPE: 8, 0.50276, 0.50276, 0.70905, 0.30337, -51.65%
Time spent: 10.41s
- Epoch 009, ExpID 34852
Train - Loss (one batch): 0.40259
Val - Loss, MSE, RMSE, MAE, MAPE: 0.70191, 0.70191, 0.83780, 0.36080, -74.91%
Test - Best epoch, Loss, MSE, RMSE, MAE, MAPE: 9, 0.50775, 0.50775, 0.71257, 0.34031, -73.31%
Time spent: 10.36s
- Epoch 010, ExpID 34852
Train - Loss (one batch): 0.52818
Val - Loss, MSE, RMSE, MAE, MAPE: 0.69595, 0.69595, 0.83424, 0.33087, -54.29%
Test - Best epoch, Loss, MSE, RMSE, MAE, MAPE: 10, 0.50603, 0.50603, 0.71136, 0.31021, -52.30%
Time spent: 10.40s
- Epoch 011, ExpID 34852
Train - Loss (one batch): 0.72140
Val - Loss, MSE, RMSE, MAE, MAPE: 0.68896, 0.68896, 0.83004, 0.32726, -53.49%
Test - Best epoch, Loss, MSE, RMSE, MAE, MAPE: 11, 0.50436, 0.50436, 0.71019, 0.30481, -51.96%
Time spent: 10.34s
- Epoch 012, ExpID 34852
Train - Loss (one batch): 0.24946
Val - Loss, MSE, RMSE, MAE, MAPE: 0.69233, 0.69233, 0.83206, 0.33264, -57.23%
Test - Best epoch, Loss, MSE, RMSE, MAE, MAPE: 11, 0.50436, 0.50436, 0.71019, 0.30481, -51.96%
Time spent: 8.56s
- Epoch 013, ExpID 34852
Train - Loss (one batch): 0.44394
Val - Loss, MSE, RMSE, MAE, MAPE: 0.68554, 0.68554, 0.82797, 0.34736, -70.69%
Test - Best epoch, Loss, MSE, RMSE, MAE, MAPE: 13, 0.50974, 0.50974, 0.71396, 0.32488, -67.92%
Time spent: 10.37s
- Epoch 014, ExpID 34852
Train - Loss (one batch): 0.48404
Val - Loss, MSE, RMSE, MAE, MAPE: 0.68675, 0.68675, 0.82870, 0.32714, -52.76%
Test - Best epoch, Loss, MSE, RMSE, MAE, MAPE: 13, 0.50974, 0.50974, 0.71396, 0.32488, -67.92%
Time spent: 8.54s
- Epoch 015, ExpID 34852
Train - Loss (one batch): 1.17063
Val - Loss, MSE, RMSE, MAE, MAPE: 0.68864, 0.68864, 0.82985, 0.33461, -62.74%
Test - Best epoch, Loss, MSE, RMSE, MAE, MAPE: 13, 0.50974, 0.50974, 0.71396, 0.32488, -67.92%
Time spent: 8.59s
- Epoch 016, ExpID 34852
Train - Loss (one batch): 0.44899
Val - Loss, MSE, RMSE, MAE, MAPE: 0.67905, 0.67905, 0.82404, 0.33528, -61.24%
Test - Best epoch, Loss, MSE, RMSE, MAE, MAPE: 16, 0.50208, 0.50208, 0.70857, 0.31353, -58.22%
Time spent: 10.43s
- Epoch 017, ExpID 34852
Train - Loss (one batch): 0.27589
Val - Loss, MSE, RMSE, MAE, MAPE: 0.69211, 0.69211, 0.83193, 0.31476, -45.67%
Test - Best epoch, Loss, MSE, RMSE, MAE, MAPE: 16, 0.50208, 0.50208, 0.70857, 0.31353, -58.22%
Time spent: 8.59s
- Epoch 018, ExpID 34852
Train - Loss (one batch): 0.59159
Val - Loss, MSE, RMSE, MAE, MAPE: 0.68085, 0.68085, 0.82514, 0.33945, -63.76%
Test - Best epoch, Loss, MSE, RMSE, MAE, MAPE: 16, 0.50208, 0.50208, 0.70857, 0.31353, -58.22%
Time spent: 8.58s
- Epoch 019, ExpID 34852
Train - Loss (one batch): 0.88143
Val - Loss, MSE, RMSE, MAE, MAPE: 0.68821, 0.68821, 0.82958, 0.32832, -48.30%
Test - Best epoch, Loss, MSE, RMSE, MAE, MAPE: 16, 0.50208, 0.50208, 0.70857, 0.31353, -58.22%
Time spent: 8.61s
- Epoch 020, ExpID 34852
Train - Loss (one batch): 0.41722
Val - Loss, MSE, RMSE, MAE, MAPE: 0.68856, 0.68856, 0.82980, 0.32707, -52.31%
Test - Best epoch, Loss, MSE, RMSE, MAE, MAPE: 16, 0.50208, 0.50208, 0.70857, 0.31353, -58.22%
Time spent: 8.57s
- Epoch 021, ExpID 34852
Train - Loss (one batch): 0.40770
Val - Loss, MSE, RMSE, MAE, MAPE: 0.68720, 0.68720, 0.82898, 0.32327, -54.01%
Test - Best epoch, Loss, MSE, RMSE, MAE, MAPE: 16, 0.50208, 0.50208, 0.70857, 0.31353, -58.22%
Time spent: 8.67s
- Epoch 022, ExpID 34852
Train - Loss (one batch): 0.59917
Val - Loss, MSE, RMSE, MAE, MAPE: 0.69841, 0.69841, 0.83571, 0.33213, -58.78%
Test - Best epoch, Loss, MSE, RMSE, MAE, MAPE: 16, 0.50208, 0.50208, 0.70857, 0.31353, -58.22%
Time spent: 8.59s
- Epoch 023, ExpID 34852
Train - Loss (one batch): 0.30084
Val - Loss, MSE, RMSE, MAE, MAPE: 0.68258, 0.68258, 0.82619, 0.32897, -56.92%
Test - Best epoch, Loss, MSE, RMSE, MAE, MAPE: 16, 0.50208, 0.50208, 0.70857, 0.31353, -58.22%
Time spent: 8.61s
- Epoch 024, ExpID 34852
Train - Loss (one batch): 0.34153
Val - Loss, MSE, RMSE, MAE, MAPE: 0.70238, 0.70238, 0.83808, 0.32616, -54.50%
Test - Best epoch, Loss, MSE, RMSE, MAE, MAPE: 16, 0.50208, 0.50208, 0.70857, 0.31353, -58.22%
Time spent: 8.57s
- Epoch 025, ExpID 34852
Train - Loss (one batch): 0.67434
Val - Loss, MSE, RMSE, MAE, MAPE: 0.67839, 0.67839, 0.82364, 0.32439, -58.92%
Test - Best epoch, Loss, MSE, RMSE, MAE, MAPE: 25, 0.50275, 0.50275, 0.70905, 0.30234, -55.65%
Time spent: 10.36s
- Epoch 026, ExpID 34852
Train - Loss (one batch): 1.05299
Val - Loss, MSE, RMSE, MAE, MAPE: 0.65866, 0.65866, 0.81158, 0.34315, -65.21%
Test - Best epoch, Loss, MSE, RMSE, MAE, MAPE: 26, 0.50320, 0.50320, 0.70937, 0.32164, -63.05%
Time spent: 10.32s
- Epoch 027, ExpID 34852
Train - Loss (one batch): 0.69315
Val - Loss, MSE, RMSE, MAE, MAPE: 0.67401, 0.67401, 0.82098, 0.33090, -57.29%
Test - Best epoch, Loss, MSE, RMSE, MAE, MAPE: 26, 0.50320, 0.50320, 0.70937, 0.32164, -63.05%
Time spent: 8.61s
- Epoch 028, ExpID 34852
Train - Loss (one batch): 0.59682
Val - Loss, MSE, RMSE, MAE, MAPE: 0.72464, 0.72464, 0.85126, 0.31550, -45.79%
Test - Best epoch, Loss, MSE, RMSE, MAE, MAPE: 26, 0.50320, 0.50320, 0.70937, 0.32164, -63.05%
Time spent: 8.61s
- Epoch 029, ExpID 34852
Train - Loss (one batch): 1.11151
Val - Loss, MSE, RMSE, MAE, MAPE: 0.70867, 0.70867, 0.84182, 0.33225, -58.24%
Test - Best epoch, Loss, MSE, RMSE, MAE, MAPE: 26, 0.50320, 0.50320, 0.70937, 0.32164, -63.05%
Time spent: 8.54s
- Epoch 030, ExpID 34852
Train - Loss (one batch): 0.80425
Val - Loss, MSE, RMSE, MAE, MAPE: 0.68474, 0.68474, 0.82749, 0.33493, -61.64%
Test - Best epoch, Loss, MSE, RMSE, MAE, MAPE: 26, 0.50320, 0.50320, 0.70937, 0.32164, -63.05%
Time spent: 8.58s
- Epoch 031, ExpID 34852
Train - Loss (one batch): 0.44118
Val - Loss, MSE, RMSE, MAE, MAPE: 0.70289, 0.70289, 0.83839, 0.32532, -53.39%
Test - Best epoch, Loss, MSE, RMSE, MAE, MAPE: 26, 0.50320, 0.50320, 0.70937, 0.32164, -63.05%
Time spent: 8.68s
- Epoch 032, ExpID 34852
Train - Loss (one batch): 0.21513
Val - Loss, MSE, RMSE, MAE, MAPE: 0.69887, 0.69887, 0.83598, 0.32724, -57.91%
Test - Best epoch, Loss, MSE, RMSE, MAE, MAPE: 26, 0.50320, 0.50320, 0.70937, 0.32164, -63.05%
Time spent: 8.67s
- Epoch 033, ExpID 34852
Train - Loss (one batch): 0.27610
Val - Loss, MSE, RMSE, MAE, MAPE: 0.70094, 0.70094, 0.83722, 0.32837, -57.44%
Test - Best epoch, Loss, MSE, RMSE, MAE, MAPE: 26, 0.50320, 0.50320, 0.70937, 0.32164, -63.05%
Time spent: 8.59s
- Epoch 034, ExpID 34852
Train - Loss (one batch): 0.40477
Val - Loss, MSE, RMSE, MAE, MAPE: 0.71048, 0.71048, 0.84290, 0.33221, -58.87%
Test - Best epoch, Loss, MSE, RMSE, MAE, MAPE: 26, 0.50320, 0.50320, 0.70937, 0.32164, -63.05%
Time spent: 8.62s
- Epoch 035, ExpID 34852
Train - Loss (one batch): 0.42582
Val - Loss, MSE, RMSE, MAE, MAPE: 0.69830, 0.69830, 0.83564, 0.32622, -56.43%
Test - Best epoch, Loss, MSE, RMSE, MAE, MAPE: 26, 0.50320, 0.50320, 0.70937, 0.32164, -63.05%
Time spent: 8.64s
- Epoch 036, ExpID 34852
Train - Loss (one batch): 0.95663
Val - Loss, MSE, RMSE, MAE, MAPE: 0.69812, 0.69812, 0.83554, 0.34979, -73.81%
Test - Best epoch, Loss, MSE, RMSE, MAE, MAPE: 26, 0.50320, 0.50320, 0.70937, 0.32164, -63.05%
Time spent: 8.60s
/home/gujuluwa/TPatchGNN/t-PatchGNN/tPatchGNN/run_models.py
2025-10-14 22:10:37
tPatchGNN/run_models.py --dataset ushcn --state def --history 24 --patience 10 --batch_size 192 --lr 1e-3 --patch_size 2 --stride 2 --nhead 1 --tf_layer 1 --nlayer 1 --te_dim 10 --node_dim 10 --hid_dim 32 --outlayer Linear --seed 5 --gpu 0
Namespace(state='def', n=100000000, hop=1, nhead=1, tf_layer=1, nlayer=1, epoch=1000, patience=10, history=24, patch_size=2.0, stride=2.0, logmode='a', lr=0.001, w_decay=0.0, batch_size=192, save='experiments/', load=None, seed=5, dataset='ushcn', quantization=0.0, model='tPatchGNN', outlayer='Linear', hid_dim=32, te_dim=10, node_dim=10, gpu='0', npatch=12, device=device(type='cuda'), PID=2757924, n_months=48, pred_window=1, ndim=5)
- Epoch 000, ExpID 34389
Train - Loss (one batch): 0.65980
Val - Loss, MSE, RMSE, MAE, MAPE: 0.82893, 0.82893, 0.91045, 0.37856, -77.84%
Test - Best epoch, Loss, MSE, RMSE, MAE, MAPE: 0, 0.52514, 0.52514, 0.72467, 0.35182, -76.92%
Time spent: 11.22s
- Epoch 001, ExpID 34389
Train - Loss (one batch): 0.31695
Val - Loss, MSE, RMSE, MAE, MAPE: 0.78824, 0.78824, 0.88783, 0.36969, -74.93%
Test - Best epoch, Loss, MSE, RMSE, MAE, MAPE: 1, 0.51944, 0.51944, 0.72072, 0.34402, -72.67%
Time spent: 10.44s
- Epoch 002, ExpID 34389
Train - Loss (one batch): 0.30500
Val - Loss, MSE, RMSE, MAE, MAPE: 0.75469, 0.75469, 0.86873, 0.34511, -59.69%
Test - Best epoch, Loss, MSE, RMSE, MAE, MAPE: 2, 0.51328, 0.51328, 0.71643, 0.32064, -57.67%
Time spent: 10.44s
- Epoch 003, ExpID 34389
Train - Loss (one batch): 0.23889
Val - Loss, MSE, RMSE, MAE, MAPE: 0.74110, 0.74110, 0.86087, 0.36196, -74.05%
Test - Best epoch, Loss, MSE, RMSE, MAE, MAPE: 3, 0.51128, 0.51128, 0.71504, 0.33740, -70.99%
Time spent: 10.53s
- Epoch 004, ExpID 34389
Train - Loss (one batch): 0.33700
Val - Loss, MSE, RMSE, MAE, MAPE: 0.74153, 0.74153, 0.86112, 0.34454, -62.33%
Test - Best epoch, Loss, MSE, RMSE, MAE, MAPE: 3, 0.51128, 0.51128, 0.71504, 0.33740, -70.99%
Time spent: 8.61s
- Epoch 005, ExpID 34389
Train - Loss (one batch): 0.51266
Val - Loss, MSE, RMSE, MAE, MAPE: 0.74958, 0.74958, 0.86578, 0.35705, -68.29%
Test - Best epoch, Loss, MSE, RMSE, MAE, MAPE: 3, 0.51128, 0.51128, 0.71504, 0.33740, -70.99%
Time spent: 8.59s
- Epoch 006, ExpID 34389
Train - Loss (one batch): 0.30398
Val - Loss, MSE, RMSE, MAE, MAPE: 0.72129, 0.72129, 0.84929, 0.34094, -61.83%
Test - Best epoch, Loss, MSE, RMSE, MAE, MAPE: 6, 0.51423, 0.51423, 0.71710, 0.31796, -59.72%
Time spent: 10.34s
- Epoch 007, ExpID 34389
Train - Loss (one batch): 0.32404
Val - Loss, MSE, RMSE, MAE, MAPE: 0.70580, 0.70580, 0.84012, 0.36940, -80.97%
Test - Best epoch, Loss, MSE, RMSE, MAE, MAPE: 7, 0.50782, 0.50782, 0.71262, 0.34674, -78.87%
Time spent: 10.40s
- Epoch 008, ExpID 34389
Train - Loss (one batch): 0.26131
Val - Loss, MSE, RMSE, MAE, MAPE: 0.71934, 0.71934, 0.84814, 0.33521, -59.10%
Test - Best epoch, Loss, MSE, RMSE, MAE, MAPE: 7, 0.50782, 0.50782, 0.71262, 0.34674, -78.87%
Time spent: 8.65s
- Epoch 009, ExpID 34389
Train - Loss (one batch): 0.55742
Val - Loss, MSE, RMSE, MAE, MAPE: 0.72854, 0.72854, 0.85355, 0.35686, -74.86%
Test - Best epoch, Loss, MSE, RMSE, MAE, MAPE: 7, 0.50782, 0.50782, 0.71262, 0.34674, -78.87%
Time spent: 8.65s
- Epoch 010, ExpID 34389
Train - Loss (one batch): 0.34569
Val - Loss, MSE, RMSE, MAE, MAPE: 0.72691, 0.72691, 0.85259, 0.33467, -56.98%
Test - Best epoch, Loss, MSE, RMSE, MAE, MAPE: 7, 0.50782, 0.50782, 0.71262, 0.34674, -78.87%
Time spent: 8.62s
- Epoch 011, ExpID 34389
Train - Loss (one batch): 0.38209
Val - Loss, MSE, RMSE, MAE, MAPE: 0.70445, 0.70445, 0.83932, 0.35159, -69.34%
Test - Best epoch, Loss, MSE, RMSE, MAE, MAPE: 11, 0.50197, 0.50197, 0.70850, 0.32926, -67.63%
Time spent: 10.34s
- Epoch 012, ExpID 34389
Train - Loss (one batch): 0.33138
Val - Loss, MSE, RMSE, MAE, MAPE: 0.69143, 0.69143, 0.83152, 0.33077, -55.63%
Test - Best epoch, Loss, MSE, RMSE, MAE, MAPE: 12, 0.50371, 0.50371, 0.70973, 0.30646, -52.53%
Time spent: 10.33s
- Epoch 013, ExpID 34389
Train - Loss (one batch): 0.56539
Val - Loss, MSE, RMSE, MAE, MAPE: 0.68653, 0.68653, 0.82857, 0.32677, -57.55%
Test - Best epoch, Loss, MSE, RMSE, MAE, MAPE: 13, 0.49934, 0.49934, 0.70664, 0.30342, -54.51%
Time spent: 10.46s
- Epoch 014, ExpID 34389
Train - Loss (one batch): 0.23826
Val - Loss, MSE, RMSE, MAE, MAPE: 0.71150, 0.71150, 0.84350, 0.33002, -55.77%
Test - Best epoch, Loss, MSE, RMSE, MAE, MAPE: 13, 0.49934, 0.49934, 0.70664, 0.30342, -54.51%
Time spent: 8.64s
- Epoch 015, ExpID 34389
Train - Loss (one batch): 0.33029
Val - Loss, MSE, RMSE, MAE, MAPE: 0.69795, 0.69795, 0.83543, 0.33376, -57.51%
Test - Best epoch, Loss, MSE, RMSE, MAE, MAPE: 13, 0.49934, 0.49934, 0.70664, 0.30342, -54.51%
Time spent: 8.60s
- Epoch 016, ExpID 34389
Train - Loss (one batch): 0.54055
Val - Loss, MSE, RMSE, MAE, MAPE: 0.68582, 0.68582, 0.82814, 0.33439, -59.00%
Test - Best epoch, Loss, MSE, RMSE, MAE, MAPE: 16, 0.49739, 0.49739, 0.70526, 0.31131, -56.49%
Time spent: 10.38s
- Epoch 017, ExpID 34389
Train - Loss (one batch): 0.57879
Val - Loss, MSE, RMSE, MAE, MAPE: 0.70903, 0.70903, 0.84204, 0.34170, -65.29%
Test - Best epoch, Loss, MSE, RMSE, MAE, MAPE: 16, 0.49739, 0.49739, 0.70526, 0.31131, -56.49%
Time spent: 8.66s
- Epoch 018, ExpID 34389
Train - Loss (one batch): 0.56702
Val - Loss, MSE, RMSE, MAE, MAPE: 0.70951, 0.70951, 0.84232, 0.32779, -55.90%
Test - Best epoch, Loss, MSE, RMSE, MAE, MAPE: 16, 0.49739, 0.49739, 0.70526, 0.31131, -56.49%
Time spent: 8.68s
- Epoch 019, ExpID 34389
Train - Loss (one batch): 0.51659
Val - Loss, MSE, RMSE, MAE, MAPE: 0.71444, 0.71444, 0.84525, 0.32045, -46.94%
Test - Best epoch, Loss, MSE, RMSE, MAE, MAPE: 16, 0.49739, 0.49739, 0.70526, 0.31131, -56.49%
Time spent: 8.67s
- Epoch 020, ExpID 34389
Train - Loss (one batch): 0.25018
Val - Loss, MSE, RMSE, MAE, MAPE: 0.69449, 0.69449, 0.83336, 0.32630, -53.41%
Test - Best epoch, Loss, MSE, RMSE, MAE, MAPE: 16, 0.49739, 0.49739, 0.70526, 0.31131, -56.49%
Time spent: 8.62s
- Epoch 021, ExpID 34389
Train - Loss (one batch): 0.34271
Val - Loss, MSE, RMSE, MAE, MAPE: 0.70323, 0.70323, 0.83859, 0.31766, -45.62%
Test - Best epoch, Loss, MSE, RMSE, MAE, MAPE: 16, 0.49739, 0.49739, 0.70526, 0.31131, -56.49%
Time spent: 8.73s
- Epoch 022, ExpID 34389
Train - Loss (one batch): 0.40582
Val - Loss, MSE, RMSE, MAE, MAPE: 0.70143, 0.70143, 0.83751, 0.33182, -59.44%
Test - Best epoch, Loss, MSE, RMSE, MAE, MAPE: 16, 0.49739, 0.49739, 0.70526, 0.31131, -56.49%
Time spent: 8.67s
- Epoch 023, ExpID 34389
Train - Loss (one batch): 1.35729
Val - Loss, MSE, RMSE, MAE, MAPE: 0.70148, 0.70148, 0.83755, 0.32676, -55.43%
Test - Best epoch, Loss, MSE, RMSE, MAE, MAPE: 16, 0.49739, 0.49739, 0.70526, 0.31131, -56.49%
Time spent: 8.68s
- Epoch 024, ExpID 34389
Train - Loss (one batch): 0.28103
Val - Loss, MSE, RMSE, MAE, MAPE: 0.72744, 0.72744, 0.85290, 0.32193, -52.45%
Test - Best epoch, Loss, MSE, RMSE, MAE, MAPE: 16, 0.49739, 0.49739, 0.70526, 0.31131, -56.49%
Time spent: 8.65s
- Epoch 025, ExpID 34389
Train - Loss (one batch): 0.40790
Val - Loss, MSE, RMSE, MAE, MAPE: 0.70764, 0.70764, 0.84121, 0.32481, -52.87%
Test - Best epoch, Loss, MSE, RMSE, MAE, MAPE: 16, 0.49739, 0.49739, 0.70526, 0.31131, -56.49%
Time spent: 8.64s
- Epoch 026, ExpID 34389
Train - Loss (one batch): 0.52733
Val - Loss, MSE, RMSE, MAE, MAPE: 0.69485, 0.69485, 0.83358, 0.32389, -48.72%
Test - Best epoch, Loss, MSE, RMSE, MAE, MAPE: 16, 0.49739, 0.49739, 0.70526, 0.31131, -56.49%
Time spent: 8.58s
